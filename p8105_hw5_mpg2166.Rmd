---
title: "P8105 Homework 5"
author: "Madison Goldrich"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 


## Problem 2

For this problem, we are using data from longitudinal study that included a control arm and an experimental arm.

Start with a dataframe containing all file names:

```{r}
file_names_df =
  as_tibble(list.files(path = "data/prob2", full.names = TRUE))
```

Iterate over file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe:

```{r}
longitudinal_df =
  file_names_df |> 
  mutate(data = map_dfr(file_names_df, read_csv))
```

Tidy the result:

```{r}
longitudinal_df =
  longitudinal_df |> 
  mutate(
    arm = str_extract(value, "(con|exp)"),
    subject_id = str_extract(value, "(01|02|03|04|05|06|07|08|09|10)"),
    week_1 = data$week_1,
    week_2 = data$week_2,
    week_3 = data$week_3,
    week_4 = data$week_4,
    week_5 = data$week_5,
    week_6 = data$week_6,
    week_7 = data$week_7,
    week_8 = data$week_8,
  ) |> 
  select(arm, subject_id, week_1:week_8) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "obs_week",
    values_to = "obs_value"
  ) |> 
  mutate(obs_week = str_replace(obs_week, "week_", ""))
```

I pivoted longer in order to make the below spaghetti plot!

```{r}
longitudinal_df |> 
  ggplot(aes(x = obs_week, y = obs_value, color = subject_id)) +
  geom_line(aes(group = subject_id)) +
  facet_grid(. ~ arm) +
  labs(
    title = "Spaghetti plot of each subject observations over time by experimental arm")
```

The observations of the subjects in the control arm appeared to stay fairly consistent over the 8 week observation period, with some fluctuation. The subjects in the experimental group saw a general increase in the measured observation value over the 8 week period. We can infer that the experimental exposure has a positive association with the outcome of interest.


## Problem 3

